"""
context_keeper.py
-------------------
åŠŸèƒ½èªªæ˜ï¼š
  å»ºç«‹ä¸Šä¸‹æ–‡å°è©±è¨˜æ†¶ï¼Œä½¿ AI å›æ‡‰æ›´é€£è²«ã€‚
  æ˜¯ã€Œæ³•è„ˆæµè½‰ã€çš„ç·´åŠŸå¯¦æˆ°ç¯„ä¾‹ã€‚

ä¿®ä»™æ¯”å–»ï¼š
  æ³•è„ˆæµè½‰ â€”â€” ä¸¹ç”°éˆæ°£æ²¿ç¶“è„ˆæµè½‰ï¼ŒåŠ›é‡ç©©å®šè€ŒæŒä¹…ã€‚
  é€™è£¡çš„ä¸Šä¸‹æ–‡è¨˜æ†¶å°±åƒæ³•è„ˆï¼Œä¿æŒå°è©±éˆåŠ›é€£çºŒï¼Œé¿å…æ–·è£‚ã€‚
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# ğŸŒ± æ–½å±•æ³•é™£ï¼Œè¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# éˆæ ¹ï¼ˆAPI Keyï¼‰ã€åŠŸæ³•ï¼ˆæ¨¡å‹ï¼‰ã€å®—é–€ä¹‹æºï¼ˆBase URLï¼‰
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
LLM_MODEL = os.getenv("LLM_MODEL", "llama-3.1-8b-instant")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE", "https://api.groq.com/openai/v1")

if not GROQ_API_KEY:
    raise ValueError("âŒ æ²’æœ‰è®€åˆ° GROQ_API_KEYï¼Œè«‹æª¢æŸ¥ .env æˆ–ç’°å¢ƒè®Šæ•¸")

# å¬å–šã€Œéˆæ ¹ã€â€”â€” åˆå§‹åŒ– LLM å®¢æˆ¶ç«¯
llm = ChatOpenAI(
    model=LLM_MODEL,
    temperature=0.7,
    api_key=GROQ_API_KEY,
    base_url=OPENAI_API_BASE
)

# æ³•è„ˆè¨˜æ†¶åº«ï¼ˆä¸Šä¸‹æ–‡åˆ—è¡¨ï¼‰
memory = []

def converse_with_memory(user_input):
    """
    AI å›æ‡‰ä½¿ç”¨è€…ï¼ŒåŒæ™‚ä¿å­˜ä¸Šä¸‹æ–‡
    """
    # å°‡ä¸Šä¸‹æ–‡èˆ‡æ–°è¼¸å…¥çµ„åˆæˆ prompt
    context_text = "\n".join(memory + [f"ä½¿ç”¨è€…ï¼š{user_input}"])
    response = llm.invoke(context_text)
    # ä¿å­˜æ–°çš„ä¸Šä¸‹æ–‡
    memory.append(f"ä½¿ç”¨è€…ï¼š{user_input}")
    memory.append(f"AIï¼š{response.content}")
    return response.content

if __name__ == "__main__":
    print("ğŸ§™ é–‹å§‹æ³•è„ˆæµè½‰ï¼šä¸Šä¸‹æ–‡å°è©±è¨˜æ†¶å•Ÿå‹•\n")
    
    while True:
        user_input = input("ğŸ‘¤ è«‹èªªè©±ï¼ˆè¼¸å…¥ exit é›¢é–‹ï¼‰ï¼š")
        if user_input.lower() == "exit":
            print("ğŸ”š æ³•è„ˆæš«æ­‡ï¼Œå°è©±è¨˜æ†¶ä¿å­˜å®Œç•¢ã€‚")
            break

        output = converse_with_memory(user_input)
        print("ğŸ§™ éˆæ™ºå›æ‡‰ï¼š", output)
        print("ğŸ“œ ç›®å‰æ³•è„ˆè¨˜æ†¶ï¼š", memory)
        print("-" * 40)
