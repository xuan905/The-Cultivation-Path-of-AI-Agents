"""
rag_knowledge.py
å®‰è£å¥—ä»¶
pip install langchain-openai scikit-learn

--------------------
åŠŸèƒ½èªªæ˜ï¼š
  ä½¿ç”¨ RAG (Retrieval-Augmented Generation) æŠ€è¡“ï¼Œ
  å¾å¤–éƒ¨çŸ¥è­˜åº«æª¢ç´¢è³‡è¨Šï¼Œå†äº¤çµ¦ LLM å›ç­”ã€‚

ä¿®ä»™æ¯”å–»ï¼š
  - RAG = èšéˆ
  - ä¿®å£«å…ˆå¸ç´å¤©åœ°éˆæ°£ï¼ˆæª¢ç´¢çŸ¥è­˜ï¼‰ï¼Œ
    å†ä»¥è‡ªèº«éˆæ ¹ï¼ˆLLMï¼‰é‹è½‰åŠŸæ³•ï¼ŒåŒ–ç‚ºè¡“æ³•ï¼ˆç­”æ¡ˆï¼‰ã€‚


ä½¿ç”¨ç¯„ä¾‹
ğŸŒŒ èšéˆé–‹å§‹ï¼šRAG ä¿®ä»™çŸ¥è­˜æª¢ç´¢

ğŸ‘¤ å•é¡Œï¼ˆè¼¸å…¥ exit çµæŸï¼‰ï¼šç¯‰åŸºæ€éº¼çªç ´ï¼Ÿ
ğŸ§™ å›æ‡‰ï¼šä¿®å£«è‹¥æ¬²ç¯‰åŸºï¼Œé ˆå°‡éˆæ°£å‡æ–¼ä¸¹ç”°ï¼Œå¦‚ç¯‰é«˜æ¨“ä¹‹åŸºçŸ³ã€‚è‹¥æ ¹åŸºä¸ç©©ï¼Œå‰‡å¾ŒçºŒä¿®è¡Œå¦‚æµ®æ²™å»ºå¡”ï¼Œå¿…ç„¶åå¡Œã€‚

ğŸ‘¤ å•é¡Œï¼ˆè¼¸å…¥ exit çµæŸï¼‰ï¼šåŠä¿®çš„ä¿®ç…‰ä¹‹é“ï¼Ÿ
ğŸ§™ å›æ‡‰ï¼šåŠä¿®ä»¥åŠç‚ºé“ï¼Œä¿®ç…‰åŠæ„å¯ç ´è¬æ³•ã€‚ç•¶ä¸€åŠå‡ºé˜ï¼Œå¤©åœ°çš†ç‚ºå…¶åŠé‹’æ‰€æŒ‡ã€‚    

ğŸ“Œ ç‰¹é»ï¼š

æ¨¡æ“¬ã€ŒAI èšéˆå¸ç´å¤©åœ°éˆæ°£ã€ â†’ å¾çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œå…§å®¹

çµåˆ LLM â†’ ç”Ÿæˆä»™ä¿ é¢¨æ ¼å›ç­”

æ”¯æ´æ›¿æ›æˆ çœŸå¯¦çŸ¥è­˜åº«ï¼ˆæª”æ¡ˆ / PDF / å‘é‡è³‡æ–™åº«ï¼‰


"""


import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ğŸŒ± è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
LLM_MODEL = os.getenv("LLM_MODEL", "llama-3.1-8b-instant")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE", "https://api.groq.com/openai/v1")

if not GROQ_API_KEY:
    raise ValueError("âŒ æ²’æœ‰è®€åˆ° GROQ_API_KEYï¼Œè«‹æª¢æŸ¥ .env æˆ–ç’°å¢ƒè®Šæ•¸")

# ğŸ§™ åˆå§‹åŒ– LLM
llm = ChatOpenAI(
    model=LLM_MODEL,
    temperature=0.3,
    api_key=GROQ_API_KEY,
    base_url=OPENAI_API_BASE
)

# ğŸ“– æ¨¡æ“¬å¤–éƒ¨çŸ¥è­˜åº«
documents = [
    "ä¿®å£«è‹¥è¦çªç ´ç¯‰åŸºï¼Œéœ€å‡èšéˆæ°£æ–¼ä¸¹ç”°ï¼Œæ‰“ä¸‹æ ¹åŸºã€‚",
    "ä¸¹è—¥èƒ½è¼”åŠ©ä¿®è¡Œï¼Œä½†éœ€æ­é…æ­£ç¢ºå¿ƒæ³•ï¼Œå¦å‰‡åå™¬ã€‚",
    "åŠä¿®ä»¥åŠç‚ºé“ï¼Œä¿®ç…‰åŠæ„å¯ç ´è¬æ³•ã€‚",
    "ç¬¦ä¿®ä»¥ç¬¦ç‚ºå¼•ï¼Œå€Ÿå¤©åœ°ä¹‹åŠ›æ–½å±•ç¬¦è¡“ã€‚",
    "ç…‰é«”ä¿®å£«é›éŠè‚‰èº«ï¼Œä»¥è¡€è‚‰ç‚ºçˆï¼ŒåŠ›å¯æ’¼å±±æ²³ã€‚"
]

# ğŸ” å»ºç«‹å‘é‡åŒ–å·¥å…·
vectorizer = TfidfVectorizer()
doc_vectors = vectorizer.fit_transform(documents)

def rag_answer(query: str):
    """ç”¨ TF-IDF + cosine similarity æ¨¡æ“¬ RAG"""
    query_vec = vectorizer.transform([query])
    sims = cosine_similarity(query_vec, doc_vectors).flatten()
    best_idx = sims.argsort()[-2:][::-1]  # å–æœ€ç›¸é—œçš„å‰2å¥

    context = "\n".join([documents[i] for i in best_idx])
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¿®ä»™æ™ºè€…ã€‚
ä»¥ä¸‹æ˜¯ä½ å¯å¼•ç”¨çš„ä¿®ä»™å…¸ç±ï¼š
{context}

å•é¡Œï¼š{query}

è«‹æ ¹æ“šå…¸ç±å…§å®¹å›ç­”ï¼Œä¸¦ç”¨ä»™ä¿ é¢¨æ ¼æ•˜è¿°ã€‚
"""
    response = llm.invoke(prompt)
    return response.content

if __name__ == "__main__":
    print("ğŸŒŒ èšéˆé–‹å§‹ï¼šç°¡æ˜“ RAG ä¿®ä»™çŸ¥è­˜æª¢ç´¢\n")
    while True:
        q = input("ğŸ‘¤ å•é¡Œï¼ˆè¼¸å…¥ exit çµæŸï¼‰ï¼š")
        if q.lower() == "exit":
            print("ğŸ”š èšéˆçµæŸï¼Œéˆæ°£æ•£å»ã€‚")
            break
        ans = rag_answer(q)
        print("ğŸ§™ å›æ‡‰ï¼š", ans)
        print("-" * 40)
